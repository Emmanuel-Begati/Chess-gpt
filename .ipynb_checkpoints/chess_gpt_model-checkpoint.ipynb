{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.cuda.device_count())  # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # GPU Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jvRrk72gq-1",
    "outputId": "e48c4c65-40ce-47d3-f7ab-aa22716aceb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt: load the gpt-2 medium model so i can use it to train my data and build a qa chat bot\n",
    "\n",
    "# !pip3 install transformers\n",
    "\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade jupyter\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (replace with your Q&A data)\n",
    "question = \"What is the capital of France?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXqhxzreg7q7",
    "outputId": "42fc6b48-d53e-4f52-c7f1-4e1ae184d377"
   },
   "outputs": [],
   "source": [
    "# prompt: I have loaded my gpt2 medium model.. i have a json file that has 'question', and 'answer' stuff... and i have .txt files that have content i scraped from wikipedia. THey are in the 'data' folder.. so i need them to be loaded and preprocessed\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "# Function to preprocess text (example)\n",
    "def preprocess_text(text):\n",
    "    # Add your text preprocessing steps here (e.g., lowercasing, removing punctuation)\n",
    "    text = text.lower()  # Example: Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Load and preprocess data from JSON file\n",
    "qa_data = []\n",
    "try:\n",
    "  with open('/content/data/chess_com_qa.json', 'r') as f:  # Replace 'your_json_file.json' with your JSON filename\n",
    "    qa_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: '/content/data/chess_com_qa.json' not found. Please make sure the file exists in the current directory or provide the correct path.\")\n",
    "\n",
    "\n",
    "# Load and preprocess data from text files\n",
    "data_dir = 'data'  # Assuming text files are in the 'data' folder\n",
    "wikipedia_content = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:  # Specify encoding if needed\n",
    "                content = file.read()\n",
    "                preprocessed_content = preprocess_text(content)\n",
    "                wikipedia_content.append(preprocessed_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or processing file '{filename}': {e}\")\n",
    "\n",
    "# Now you have preprocessed data in qa_data (JSON) and wikipedia_content (list of strings)\n",
    "\n",
    "# Example usage (replace with your actual Q&A data processing and model training)\n",
    "\n",
    "# Example: Accessing a question and answer from the JSON data\n",
    "if qa_data:  # Check if qa_data is not empty\n",
    "    question = qa_data[0].get('question', 'No question found') #safe access if key does not exist\n",
    "    answer = qa_data[0].get('answer', 'No answer found')    #safe access if key does not exist\n",
    "    print(\"Example from JSON:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "# Example: Accessing the preprocessed Wikipedia content\n",
    "if wikipedia_content:\n",
    "  print(\"\\nExample from Wikipedia:\")\n",
    "  print(f\"First Wikipedia text file content:\\n{wikipedia_content[0][:200]}...\") # Print the first 200 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "p-OkvBfTjGSL",
    "outputId": "d1272aab-0983-4731-801e-da050bcb369e"
   },
   "outputs": [],
   "source": [
    "# prompt: now use the data that we have loaded to train the model so that it will be domain specific to chess..\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "import os\n",
    "import torch   # Import torch\n",
    "\n",
    "\n",
    "# Prepare data for training (example using a simple list of text)\n",
    "train_data = []\n",
    "if qa_data:\n",
    "    for item in qa_data:\n",
    "        question = item.get('question', '')\n",
    "        answer = item.get('answer', '')\n",
    "        train_data.append(question + \" \" + answer)\n",
    "\n",
    "if wikipedia_content:\n",
    "    train_data.extend(wikipedia_content)\n",
    "\n",
    "# Tokenize the data\n",
    "# Add the padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "train_encodings = tokenizer(train_data, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Create a custom dataset\n",
    "class ChessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = ChessDataset(train_encodings)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./chess_gpt2_results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./chess_gpt2_logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./chess_gpt2_fine_tuned\")\n",
    "tokenizer.save_pretrained(\"./chess_gpt2_fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF4DfI5LAm77"
   },
   "outputs": [],
   "source": [
    "# prompt: evaluate the model and do an extensive eda of the data we loaded using matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the JSON data (assuming qa_data is already loaded as in the previous code)\n",
    "try:\n",
    "    with open('/content/data/chess_com_qa.json', 'r') as f:\n",
    "        qa_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: '/content/data/chess_com_qa.json' not found.\")\n",
    "    qa_data = [] # Initialize as empty list to avoid errors\n",
    "\n",
    "# EDA for JSON data\n",
    "if qa_data:\n",
    "    df_qa = pd.DataFrame(qa_data)\n",
    "\n",
    "    # Example 1: Question Length Distribution\n",
    "    df_qa['question_length'] = df_qa['question'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_qa['question_length'], bins=20)\n",
    "    plt.title('Distribution of Question Lengths')\n",
    "    plt.xlabel('Question Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Example 2: Answer Length Distribution\n",
    "    df_qa['answer_length'] = df_qa['answer'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_qa['answer_length'], bins=20)\n",
    "    plt.title('Distribution of Answer Lengths')\n",
    "    plt.xlabel('Answer Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Example 3: Scatter plot of question vs answer length\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(df_qa['question_length'], df_qa['answer_length'])\n",
    "    plt.title('Question Length vs Answer Length')\n",
    "    plt.xlabel('Question Length')\n",
    "    plt.ylabel('Answer Length')\n",
    "    plt.show()\n",
    "\n",
    "# Load and preprocess text data (wikipedia_content) - assuming it's available\n",
    "wikipedia_content = []\n",
    "data_dir = 'data'\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                wikipedia_content.append(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{filename}': {e}\")\n",
    "\n",
    "# EDA for Text Data\n",
    "if wikipedia_content:\n",
    "    # Example 4: Word count distribution in text files\n",
    "    word_counts = [len(text.split()) for text in wikipedia_content]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(word_counts, bins=20)\n",
    "    plt.title(\"Word count Distribution in Wikipedia Text Files\")\n",
    "    plt.xlabel(\"Number of words\")\n",
    "    plt.ylabel(\"Number of files\")\n",
    "    plt.show()\n",
    "\n",
    "# Model Evaluation (Example: Perplexity - you'll need to calculate perplexity)\n",
    "\n",
    "# Note: Replace with your actual model evaluation metrics\n",
    "\n",
    "# Assuming 'perplexity' is already calculated\n",
    "# perplexity = your_model_evaluation_metric\n",
    "# print(f\"Perplexity: {perplexity}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
